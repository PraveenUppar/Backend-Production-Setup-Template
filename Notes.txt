Phase 1: Data Modeling                              -- DONE
Phase 2: API Development                            -- DONE
Phase 3: Logging                                    -- DONE
Phase 4: Testing                                    -- 
Phase 5: Caching                                    -- DONE
Phase 6: Scaling & Deployment                       -- DONE
Phase 7: Pagination                                 -- DONE
Phase 8: Validation                                 -- DONE
Phase 9: Error Handling                             -- DONE
Phase 10: Docker                                    -- DONE
Phase 11: Kubernetes                                -- DONE
Phase 12: CI CD                                     -- DONE
Phase 13 Open API using sawgger                     -- DONE
Phase 14 Health check points                        -- DONE
Phase 15 Monitoring                                 -- 
Phase 16 Rate limiting                              -- DONE
Phase 17 Observability                              -- 

---------------------------------------------------------------------------
Request → [Logger] → [Zod Validation] → Controller → Service → Database (If anything fails) ↘ [Global Error Handler] → Response
---------------------------------------------------------------------------
5 layers: The Big Picture

TypeScript code
   ↓
Prisma Client (generated code)
   ↓
Prisma Adapter (pg / Supabase pooler)
   ↓
Postgres (Supabase)
   ↓
Docker (packaging + runtime)
---------------------------------------------------------------------------

Prisma lifecycle

schema.prisma
    ↓
prisma generate   ← build time
    ↓
TypeScript compile
    ↓
Runtime Prisma Client
---------------------------------------------------------------------------
Prisma generates a typed client from the schema.
That client must exist before TypeScript compiles.
Docker builds the app without envs, so Prisma needs a dummy DB URL at build time.
Real credentials are injected only when the container runs.
---------------------------------------------------------------------------
Prisma:
  schema → generate → types → runtime queries

TypeScript:
  source → compile → dist
---------------------------------------------------------------------------
Monitoring answers three questions:

Is the service alive? → health check 
Is it slow or failing? → request metrics
Why did it fail? → structured logs

---------------------------------------------------------------------------
You now have:

✅ Health checks (availability)
✅ Request metrics (performance)
✅ Error tracking (reliability)
✅ Structured logs (debugging)

This is observability, not logging.

What NOT to do yet

❌ Prometheus
❌ Grafana
❌ OpenTelemetry
❌ Tracing systems

Those come after infra exists.
---------------------------------------------------------------------------
Availability
/health endpoint
Docker health check

Performance

Request latency
Slow endpoints
Traffic patterns

Reliability

Error frequency
Error types
Failing routes
---------------------------------------------------------------------------
What problems CI will now catch early

✔ Missing DATABASE_URL
✔ Prisma client mismatch
✔ Broken Dockerfile
✔ TypeScript errors
✔ Missing files in image
---------------------------------------------------------------------------
What Kubernetes does now

If probe fails:

readiness → pod removed from traffic
liveness → pod restarted

You now have self-healing.
---------------------------------------------------------------------------
✔ Dockerized backend
✔ CI validation
✔ Image publishing
✔ Kubernetes Deployment
✔ Secrets
✔ Health probes
✔ Service
✔ Self-healing system
✅ Multi-replica backend
✅ Resource protection
✅ Health-based routing
✅ Zero-downtime rolling updates
✅ Instant rollback capability
Ingress routing
---------------------------------------------------------------------------
A rolling update:

“How do I replace running pods with new ones without stopping traffic?”

Kubernetes does not:

stop all old pods
then start new ones

Instead, it:

brings up new pods
waits until they’re ready
shifts traffic
then removes old pods

This only works because:

you have readiness probes
your app is stateless
traffic goes through a Service
---------------------------------------------------------------------------
How a real rolling update happens

When you deploy a new image version:
Kubernetes creates 1 new pod
Waits for readiness probe to pass
Routes traffic to it
Deletes 1 old pod
Repeats until done
---------------------------------------------------------------------------
In production, traffic looks like:

Internet
   ↓
Load Balancer / Reverse Proxy
   ↓
Ingress Controller
   ↓
Kubernetes Service
   ↓
Pods

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.96.74.233   <pending>     80:32419/TCP,443:30816/TCP   86s
ingress-nginx-controller-admission   ClusterIP      10.96.248.83   <none>        443/TCP                      86s

kubectl get ingress -n todo-backend
kubectl logs -n ingress-nginx deploy/ingress-nginx-controller

“Ingress is the entry point to the cluster.
It routes external traffic to services based on host and path.
Services abstract pod lifecycles, so rolling updates don’t cause downtime.”

Ingress is just: A smart reverse proxy running inside the cluster

In your case:

That proxy is NGINX
It runs as a Pod
It listens on ports 80 / 443
It reads Ingress rules
It forwards traffic to Services

kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
This means:

“Create a tunnel from my laptop port 8080 → ingress controller port 80 inside Kubernetes”

No Docker networking.
No NodePort.
No LoadBalancer.
Just a direct pipe.

Traffic flow in Development
curl
 ↓
todo.local → 127.0.0.1
 ↓
localhost:8080   (port-forward)
 ↓
Ingress Controller (NGINX)
 ↓
Ingress rule: Host = todo.local
 ↓
Service: todo-backend
 ↓
Healthy Pod
 ↓
Express app
 ↓
/health/database


Traffic flow in production
Internet
  ↓
DNS (api.pavii.dev → LB IP)
  ↓
Cloud LoadBalancer
  ↓
Ingress Controller
  ↓
Service
  ↓
Pods

“Ingress defines routing inside Kubernetes, but it doesn’t expose traffic by itself. In production, a cloud load balancer forwards traffic to the ingress controller. 
In local environments like kind, we use port-forwarding or node port mappings to simulate that exposure.
---------------------------------------------------------------------------
When someone asks:

“How does traffic reach your service in Kubernetes?”

Your answer:

“DNS resolves the domain to the load balancer.
The load balancer forwards traffic to an ingress controller.
Ingress routes based on host/path to a service, which load-balances across pods.”
---------------------------------------------------------------------------
Headers
Key: Host
Value: todo.local

http://localhost:8080/api/v1/auth/login

{
    "email": "pavi@gmail.com",
    "password": "pavi@123"
}

 "success": true,
    "message": "Login successful",
    "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjMwYTAyNzBhLWRjNzUtNDVhMC1iNWUwLTkyMGFhYTgzNzc2MiIsImlhdCI6MTc2ODAzMjk1OCwiZXhwIjoxNzY4MDM2NTU4fQ.uPvRo5pXEajJtPfoNCv5Pvh75p1-BOVYQcX46yZP2og",
    "user": {
        "email": "pavi@gmail.com"
    }


In production:

Port-forward = ❌ never
Cloud Load Balancer = ✅ always


Local (kind)	  Production
port-forward	  AWS ALB / GCP LB
localhost:8080	  api.todo.com
/etc/hosts	     Route53 / Cloud DNS
manual Host      header/real DNS
---------------------------------------------------------------------------
HPA 

HPA = Horizontal Pod Autoscaler

It answers one question repeatedly:

“Do I need more pods or fewer pods right now?”

It does this by watching metrics (CPU / memory / custom metrics).

kubectl get apiservices | grep metrics
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl top pods -n todo-backend

NAME                            CPU(cores)   MEMORY(bytes)   
todo-backend-5dfb449b57-954jk   1m           36Mi
todo-backend-5dfb449b57-svvdr   0m           36Mi
todo-backend-5dfb449b57-wmssf   1m           34Mi

kubectl get deploy todo-backend -n todo-backend -o yaml | grep -A10 resources

resources:
          limits:
            cpu: 500m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi

kubectl apply -f hpa.yaml
kubectl get hpa -n todo-backend

NAME               REFERENCE                 TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
todo-backend-hpa   Deployment/todo-backend   cpu: 0%/50%             2         4         3          7s

kubectl describe hpa todo-backend-hpa -n todo-backendnd

Name:                                                  todo-backend-hpa
Namespace:                                             todo-backend
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sat, 10 Jan 2026 08:25:45      
Reference:                                             Deployment/todo-backend
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 50%

---------------------------------------------------------------------------
HTTPS with cert-manager

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
kubectl get pods -n cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-859f5d5d75-8mz9z              1/1     Running   0          36s
cert-manager-cainjector-6569c5c766-p7mdm   1/1     Running   0          36s
cert-manager-webhook-54896b88d9-gnpg9      1/1     Running   0          36s

kubectl apply -f cluster-issuer.yaml
kubectl get clusterissuer

NAME               READY   AGE
letsencrypt-prod   True    6s

kubectl apply -f ingress.yaml

kubectl describe certificate -n todo-backend

